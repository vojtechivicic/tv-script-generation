{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tv-script-generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hUFBu9vbbEJB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TV Script Generation\n",
        "Pytorch implementation of **Recurrent Neural Network (LSTM)** used for generation of new Seinfeld TV scripts. The network is trained on scripts from [Seinfeld dataset](https://www.kaggle.com/thec03u5/seinfeld-chronicles#scripts.csv).\n",
        "\n",
        "This project is the fourth assigment for [Udacity Deep Learning Nanodegree](https://eu.udacity.com/course/deep-learning-nanodegree--nd101)."
      ]
    },
    {
      "metadata": {
        "id": "9d63H9EacvgS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import Dependencies"
      ]
    },
    {
      "metadata": {
        "id": "D1-UlrilczCD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vvlAaO2xbSTD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the Data\n",
        "First we load the data from file `Seinfeld_Scripts.txt` and explore it a little bit.\n"
      ]
    },
    {
      "metadata": {
        "id": "Aer1kHoybTKW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_name = 'Seinfeld_Scripts.txt'\n",
        "\n",
        "def load(file_name):\n",
        "    \"\"\"\n",
        "    Reads a file file_name and returns it as string\n",
        "    \n",
        "    :param file_name: name of a file to load\n",
        "    :return: content of the file\n",
        "    \"\"\"\n",
        "    with open(file_name , 'r') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "text = load(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PcI7dPZUb0qe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Explore the Data\n",
        "To see what the data looks like, we display some **statistics** and the beginning of the text."
      ]
    },
    {
      "metadata": {
        "id": "4mpXFA0gb5to",
        "colab_type": "code",
        "outputId": "524cbcae-6dea-402d-e999-6ef6dacb42db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print('Dataset statistics')\n",
        "print('Number of lines: {}'.format(len(text.split('\\n'))))\n",
        "print('Number of words: {}'.format(len(text.split())))\n",
        "print('Number of unique words: {}'.format(len(set(text.split()))))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset statistics\n",
            "Number of lines: 109233\n",
            "Number of words: 605614\n",
            "Number of unique words: 46367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EBDu-fde2d1a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Print a few lines:"
      ]
    },
    {
      "metadata": {
        "id": "HPZK9mGs2SbB",
        "colab_type": "code",
        "outputId": "4063109e-4b22-49a0-b899-02c3bda7a638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "print('\\n'.join(text.split('\\n')[10:20]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jerry: oh, you dont recall? \n",
            "\n",
            "george: (on an imaginary microphone) uh, no, not at this time. \n",
            "\n",
            "jerry: well, senator, id just like to know, what you knew and when you knew it. \n",
            "\n",
            "claire: mr. seinfeld. mr. costanza. \n",
            "\n",
            "george: are, are you sure this is decaf? wheres the orange indicator? \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dlcgX5E_bmo7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocess the Data\n",
        "Before building and training a network, we preprocess the data: \n",
        "- tokenize punctuation, \n",
        "- split the text into words,\n",
        "- convert words to integers."
      ]
    },
    {
      "metadata": {
        "id": "-qM1hc1wecq6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenize Punctuation\n",
        "In order not to consider words with punctuation ('hello' vs. 'hello!') as different words, the **punctuation signs are replaced** by special words."
      ]
    },
    {
      "metadata": {
        "id": "xoM_Nt0Be6CZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def punctuation_lookup():\n",
        "    \"\"\"\n",
        "    Returns a dictionary of punctuation signs and special words to replace them\n",
        "    \n",
        "    :return: a dictionary of punctuation\n",
        "    \"\"\"\n",
        "    punctuation = {\n",
        "        '.': '<Period>',\n",
        "        ',': '<Comma>',\n",
        "        '\"': '<QuotationMark>',\n",
        "        ';': '<Semicolon>',\n",
        "        '!': '<ExclamationMark>',\n",
        "        '?': '<QuestionMark>',\n",
        "        '(': '<LeftParentheses>',\n",
        "        ')': '<RightParentheses>',\n",
        "        '-': '<Dash>',\n",
        "        '\\n': '<Return>'}\n",
        "    return punctuation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KbfIYLtleaPl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Lookup Tables\n",
        "We create lookup tables - **mappings for converting words to integers** and back for use in a word embedding."
      ]
    },
    {
      "metadata": {
        "id": "AQVnD2oobw7H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_lookup_tables(text):\n",
        "    \"\"\"\n",
        "    Creates lookup tables to convert words to integers and back\n",
        "    \n",
        "    :param text: input text split into words\n",
        "    :return: two dictionaries converting words to integers and back\n",
        "    \"\"\"\n",
        "\n",
        "    counter = Counter(text)\n",
        "    # Start indexing from 1 (leave 0 as a padding word)\n",
        "    vocab_to_int = {word: i for i, (word, count) \n",
        "                    in enumerate(counter.most_common(), 1)}\n",
        "    int_to_vocab = {vocab_to_int[word]: word for word in counter}\n",
        "\n",
        "    return (vocab_to_int, int_to_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9iIqWY_zk0tG",
        "colab_type": "code",
        "outputId": "b757252c-8a16-4519-8934-a48305803da9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    Preprocesses text to be ready as an input to the neural network\n",
        "    \n",
        "    :param text: input text\n",
        "    :return: text converted to integers and two dictionaries mapping word to \n",
        "        integers and back\n",
        "    \"\"\"\n",
        "    \n",
        "    # Tokenize punctuation\n",
        "    punctuation = punctuation_lookup()\n",
        "    for sign, token in punctuation.items():\n",
        "        text = text.replace(sign, ' {} '.format(token))\n",
        "        \n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Split to words\n",
        "    text = text.split()\n",
        "    \n",
        "    # Create Lookup Tables and convert text to integers\n",
        "    vocab_to_int, int_to_vocab = create_lookup_tables(text)\n",
        "    text_int = [vocab_to_int[word] for word in text]\n",
        "\n",
        "    return text_int, vocab_to_int, int_to_vocab\n",
        "    \n",
        "text_int, vocab_to_int, int_to_vocab = preprocess(text)\n",
        "\n",
        "# Print the beginning of the processed text\n",
        "print(text_int[0:20])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8, 35, 5, 28, 19, 25, 23, 51, 59, 4, 35, 5, 28, 3, 84, 121, 63, 4, 9, 55]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q-C2fh70c3qF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build and Train the Neural Network\n",
        "Check if training on **GPU** is available."
      ]
    },
    {
      "metadata": {
        "id": "csXxCs9adMRR",
        "colab_type": "code",
        "outputId": "ba60ca24-70b6-4200-ab69-250a7d4a3865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "    print('CUDA is available! Training on GPU.')\n",
        "else:\n",
        "    print('CUDA is not available. Training on CPU.') "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available! Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lg0rIdrudWGB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Batch Input\n",
        "Split dataset to **training and validation**, prepare **batches** out of the data and create data loaders.  "
      ]
    },
    {
      "metadata": {
        "id": "2mhXja12dXvz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_data(words, seq_length, batch_size, train_split = 0.9):\n",
        "    \"\"\"\n",
        "    Makes batches of data and returns DataLoaders \n",
        "    \n",
        "    :param words: processed input text converted to integer list\n",
        "    :param seq_length: length of input sequnce\n",
        "    :param batch_size: number of examples in a batch\n",
        "    :param train_split: portion of data to be used for training\n",
        "    :return: DataLoaders with training and validation data\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define feature and target tensors\n",
        "    feature_tensor = torch.tensor([words[i:i+seq_length]\n",
        "                                   for i, _ in enumerate(words[seq_length:])])\n",
        "    target_tensor = torch.tensor([word for word in words[sequence_length:]])\n",
        "    \n",
        "    # Create dataset\n",
        "    data_all = TensorDataset(feature_tensor, target_tensor)\n",
        "    \n",
        "    # Split into training and validation parts\n",
        "    n_train = int(train_split * len(data_all))\n",
        "    data = dict(zip(['train', 'valid'], torch.utils.data.random_split(\n",
        "        data_all, (n_train, len(data_all) - n_train))))\n",
        "    \n",
        "    # Define DataLoaders\n",
        "    dataloaders = {\n",
        "        phase: DataLoader(data[phase], shuffle = True, batch_size = batch_size)\n",
        "                for phase in ['train', 'valid']}\n",
        "    \n",
        "    return dataloaders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gWKXo6HEdYhQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model Architecture\n",
        "We build the model from **Embedding** layer to transform words to embeddings (word vectors), `n` layers of **LSTM RNN** and the final **Linear** output layer."
      ]
    },
    {
      "metadata": {
        "id": "pQVJqrN7daa2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TvScriptGenerator(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, \n",
        "                 dropout = 0.5):\n",
        "        \"\"\"\n",
        "        Initializes the neural network\n",
        "        \n",
        "        :param vocab_size: size of the vocabulary\n",
        "        :param embedding_dim: dimension of word embeddings\n",
        "        :param hidden_dim: dimension of hidden state of RNN\n",
        "        :param num_layers: number of layers of RNN\n",
        "        :param dropout: dropout rate between layers\n",
        "        \n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Define layers        \n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
        "                            batch_first = True, dropout = dropout)\n",
        "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
        "        \n",
        "    def forward(self, nn_input, hidden):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        \n",
        "        :param nn_input: input of neural network\n",
        "        :param hidden: hidden state of RNN\n",
        "        :return: model output and a new hidden state\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        embedding = self.embed(nn_input)\n",
        "        \n",
        "        lstm_output, hidden = self.lstm(embedding, hidden)\n",
        "        \n",
        "        # Shape of lstm_output is [batch_size, sequence_length, hidden_dim]\n",
        "        # Take only the output for the last element of a sequence\n",
        "        lstm_output = lstm_output[:, -1, :].squeeze()\n",
        "        \n",
        "        output = self.linear(lstm_output)\n",
        "        \n",
        "        return output, hidden\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ruAWCh2xdjqq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Implement the Training Algorithm\n",
        "**Forward and backward** propagation pass:"
      ]
    },
    {
      "metadata": {
        "id": "iCcWC9lkdopl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_backward_prop(model, optimizer, criterion, inputs, targets, hidden,\n",
        "                          clip):\n",
        "    \"\"\"\n",
        "    Performs a forward nad backward propagation pass of a model for one batch\n",
        "    of data, returns training loss and new hidden state\n",
        "    \n",
        "    :param model: model to perform forward and backward propagations on \n",
        "    :param optimizer: optimizer used for updating parameters\n",
        "    :param criterion: loss function\n",
        "    :param inputs: input to the model\n",
        "    :param targets: target words to compare with predicted words\n",
        "    :param hidden: hidden state of LSTM network\n",
        "    :return: training loss and new hidden state\n",
        "    \"\"\"\n",
        "    \n",
        "    # Detach hidden state so that we don't backpropagate through entire history\n",
        "    if hidden is not None:\n",
        "        hidden = tuple([each.data for each in hidden])\n",
        "        \n",
        "    # Zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward propagation\n",
        "    output, hidden = model(inputs, hidden)\n",
        "    loss = criterion(output, targets)\n",
        "    \n",
        "    # Clip the parameters to prevent exploding gradients\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    \n",
        "    # Backward propagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return float(loss), hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xli3aoWp-HFr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training for **one epoch**:"
      ]
    },
    {
      "metadata": {
        "id": "-8ph5_4egeYk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, criterion, loaders, batch_size, clip):\n",
        "    \"\"\"\n",
        "    Trains a model for one epoch\n",
        "    \n",
        "    :param model: model to train\n",
        "    :param optimizer: optimizer for optimization of model parameters\n",
        "    :param criterion: loss function\n",
        "    :param loader: DataLoader with training data\n",
        "    :param clip: value used to clip gradients\n",
        "    :return: average training loss\n",
        "    \"\"\"\n",
        "\n",
        "    # Start each epoch with clean hidden state\n",
        "    hidden = None;\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    \n",
        "    for inputs, labels in loaders['train']:\n",
        "        # Take only full batches\n",
        "        if len(inputs) != batch_size:\n",
        "            break\n",
        "        if train_on_gpu:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Forward and backward pass\n",
        "        loss, hidden = forward_backward_prop(model, optimizer, criterion,\n",
        "                                             inputs, labels, hidden, clip)\n",
        "        train_loss += loss\n",
        "        \n",
        "    return train_loss / len(loaders['train'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LOKnHxCS-NG1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Validation:**"
      ]
    },
    {
      "metadata": {
        "id": "vkH6qy7siR0V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def validate(model, criterion, loaders, batch_size):\n",
        "    \"\"\"\n",
        "    Runs forward pass on validation data and returns validation loss\n",
        "    \n",
        "    :param model: model to validate\n",
        "    :param criterion: loss function\n",
        "    :param loaders: DataLoaders with validation data\n",
        "    :return: average validation loss\n",
        "    \"\"\"\n",
        "    \n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loaders['valid']:\n",
        "            # Take only full batches\n",
        "            if len(inputs) != batch_size:\n",
        "                break\n",
        "            if train_on_gpu:\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            output, _ = model(inputs, None)\n",
        "            valid_loss += float(criterion(output, labels))\n",
        "            \n",
        "    return valid_loss / len(loaders['valid'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W8IJ78jv9-9o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Functions for **saving and loading** model parameters:"
      ]
    },
    {
      "metadata": {
        "id": "xjoqwEgtVN5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_parameters(model, epoch, loss, path):\n",
        "    \"\"\"\n",
        "    Saves a checkpoint with state_dict of a model into a file\n",
        "    \n",
        "    :param model: model to save\n",
        "    :param epoch: epoch number\n",
        "    :param loss: validation loss of the model\n",
        "    :param path: path to save the checkpoint to\n",
        "    \"\"\"\n",
        "    checkpoint = {\n",
        "        'state_dict': model.state_dict(),\n",
        "        'epochs': epoch,\n",
        "        'loss': loss\n",
        "    }\n",
        "    torch.save(checkpoint, path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BaRlFsew6JPO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_parameters(model, path):\n",
        "    \"\"\"\n",
        "    Loads parameters into model\n",
        "    \n",
        "    :param model: model to load parameters into\n",
        "    :param path: filepath of a checkpoint with saved parameters\n",
        "    :return: epoch and minimal validation loss of the saved model\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    epoch = checkpoint['epochs']\n",
        "    min_loss = checkpoint['loss']\n",
        "    \n",
        "    return epoch, min_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AMfjUtQolCXM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training loop:**"
      ]
    },
    {
      "metadata": {
        "id": "3UpiFZhI_ZVk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion, scheduler, num_epochs, loaders, \n",
        "          batch_size, clip = 5):\n",
        "    \"\"\"\n",
        "    Trains a model\n",
        "    \n",
        "    :param model: model to train\n",
        "    :param optimizer: optimizer for optimization of model parameters\n",
        "    :param criterion: loss function\n",
        "    :param scheduler: scheduler for learning rate decay\n",
        "    :param num_epochs: number of epoch to train\n",
        "    :param loader: DataLoader with training data\n",
        "    :param batch_size: number of examples in a batch\n",
        "    :param clip: value used to clip gradients\n",
        "    \"\"\"\n",
        "    \n",
        "    min_loss = math.inf\n",
        "    \n",
        "    # Move model to GPU\n",
        "    if train_on_gpu:\n",
        "        model.cuda()\n",
        "        \n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Training\n",
        "        train_loss = train_one_epoch(model, optimizer, criterion, loaders, \n",
        "                                     batch_size, clip)\n",
        "        \n",
        "        # Validation\n",
        "        valid_loss = validate(model, criterion, loaders, batch_size)\n",
        "            \n",
        "        # Save model if validation loss decreased\n",
        "        if valid_loss < min_loss:\n",
        "            min_loss = valid_loss\n",
        "            save_parameters(model, epoch, valid_loss, 'checkpoint.pth')\n",
        "            \n",
        "        scheduler.step()\n",
        "            \n",
        "        print(('Epoch {}/{}, Training loss: {:.3f}, Validation loss: {:.3f}, ' \\\n",
        "               'Time: {:.0f} s').format(epoch, num_epochs,\n",
        "                                        train_loss, valid_loss,\n",
        "                                        time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LT2jyX7tlH0-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "v9QP0Zqe_5xh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Batching hyperparameters\n",
        "sequence_length = 32\n",
        "batch_size = 128\n",
        "\n",
        "# Training hyperparameters\n",
        "num_epochs = 15\n",
        "lr = 0.001\n",
        "dropout = 0.5\n",
        "weight_decay = 0.00001\n",
        "\n",
        "# Model hyperparameters\n",
        "vocab_size = len(vocab_to_int) + 1\n",
        "embedding_dim = 512\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "\n",
        "# Scheduler hyperparameters\n",
        "step = 12\n",
        "gamma = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XMJARmcTlOMD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the Model"
      ]
    },
    {
      "metadata": {
        "id": "HZ4D3zl6AECQ",
        "colab_type": "code",
        "outputId": "0f035484-5d08-4fc5-a097-2980eb06b3b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# Build a model\n",
        "model = TvScriptGenerator(vocab_size, embedding_dim, hidden_dim, num_layers,\n",
        "                      dropout)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = lr, \n",
        "                       weight_decay = weight_decay)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step, gamma)\n",
        "\n",
        "# Move model to GPU\n",
        "if train_on_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "# Load and batch data\n",
        "loaders = batch_data(text_int, sequence_length, batch_size)\n",
        "\n",
        "# Train\n",
        "train(model, optimizer, criterion, scheduler, num_epochs, loaders, batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15, Training loss: 4.389, Validation loss: 4.021, Time: 320 s\n",
            "Epoch 2/15, Training loss: 3.989, Validation loss: 3.904, Time: 320 s\n",
            "Epoch 3/15, Training loss: 3.882, Validation loss: 3.842, Time: 320 s\n",
            "Epoch 4/15, Training loss: 3.821, Validation loss: 3.806, Time: 320 s\n",
            "Epoch 5/15, Training loss: 3.775, Validation loss: 3.775, Time: 320 s\n",
            "Epoch 6/15, Training loss: 3.739, Validation loss: 3.753, Time: 320 s\n",
            "Epoch 7/15, Training loss: 3.706, Validation loss: 3.737, Time: 320 s\n",
            "Epoch 8/15, Training loss: 3.678, Validation loss: 3.722, Time: 321 s\n",
            "Epoch 9/15, Training loss: 3.656, Validation loss: 3.711, Time: 320 s\n",
            "Epoch 10/15, Training loss: 3.636, Validation loss: 3.701, Time: 319 s\n",
            "Epoch 11/15, Training loss: 3.619, Validation loss: 3.690, Time: 322 s\n",
            "Epoch 12/15, Training loss: 3.602, Validation loss: 3.688, Time: 322 s\n",
            "Epoch 13/15, Training loss: 3.587, Validation loss: 3.680, Time: 322 s\n",
            "Epoch 14/15, Training loss: 3.439, Validation loss: 3.629, Time: 322 s\n",
            "Epoch 15/15, Training loss: 3.407, Validation loss: 3.625, Time: 322 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YezOUrLe_r11",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the best model parameters\n",
        "load_parameters(model, 'checkpoint.pth');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RbKyBnLKdrm-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate TV Scripts\n",
        "Now we can use the trained model to generate **new Seinfeld scripts**. \n",
        "\n",
        "The words of a new script are generated **one by one** until we reach a given length. A **sequence of words** is used to obtain a next word, it's **randomly chosen** out of `topk` most likely words. In the beginning, the sequence contains only a prime word and is padded with 0."
      ]
    },
    {
      "metadata": {
        "id": "NCT2usrq__40",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_script(model, prime_id, length, topk = 5):\n",
        "    \"\"\"\n",
        "    Generates a new TV script\n",
        "    \n",
        "    :param model: model used for generating words\n",
        "    :param prime_id: prime word index\n",
        "    :param length: length of the generated script\n",
        "    :param tokp: a number of most likely words to choose the next word from\n",
        "    :return: generated TV script\n",
        "    \"\"\"\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Sequence with a starting word, padded with zeros\n",
        "        current_sequence = np.full((1, sequence_length), 0)\n",
        "        current_sequence[0, -1] = prime_id\n",
        "        \n",
        "        # List for generated indices of words\n",
        "        script_ids = [prime_id]\n",
        "\n",
        "        for _ in range(1, length):\n",
        "            # Convert to torch tensor and move to GPU\n",
        "            tensor = torch.tensor(current_sequence)\n",
        "            if train_on_gpu:\n",
        "                tensor = tensor.cuda()\n",
        "\n",
        "            # Run the model to get probabilities of the next word\n",
        "            output, _ = model(tensor, None)\n",
        "            ps = F.softmax(output, dim = 0)\n",
        "            top_ps, top_word_ids = ps.topk(topk)\n",
        "            \n",
        "            # Convert to numpy and move back to CPU\n",
        "            top_word_ids = top_word_ids.cpu().numpy()\n",
        "            top_ps = top_ps.cpu().numpy()\n",
        "            \n",
        "            # Randomly choose the next word out of top k words\n",
        "            next_word_id = np.random.choice(top_word_ids, p = top_ps/top_ps.sum())\n",
        "\n",
        "            # Update the sequence and list with the new word\n",
        "            current_sequence = np.roll(current_sequence, -1)\n",
        "            current_sequence[0, -1] = next_word_id\n",
        "            script_ids.append(next_word_id)\n",
        "\n",
        "        # Join generated word list into a string\n",
        "        script = ' '.join([int_to_vocab[word_id] for word_id in script_ids])\n",
        "        \n",
        "        # Replace the special punctuation words for actual punctuation\n",
        "        punctuation = punctuation_lookup()\n",
        "        for sign, token in punctuation.items():\n",
        "            script = script.replace(' ' + token.lower(), sign)\n",
        "        script = script.replace('( ', ' (')\n",
        "        script = script.replace('\\n ','\\n')\n",
        "            \n",
        "    return script"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5oc930lU6Ksx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now generate an example script starting with `'jerry:'`. As you can see, the script is **not perfect** and sometimes it doesn't make sense. On the other hand, it's not too bad, the model **learnt the structure** of a script pretty well and also some grammatical rules."
      ]
    },
    {
      "metadata": {
        "id": "Mv0f7sFupYrs",
        "colab_type": "code",
        "outputId": "80feea6b-5737-415b-ce44-878fe2d61f3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "cell_type": "code",
      "source": [
        "script = generate_script(model, vocab_to_int['jerry:'], 400, 10)\n",
        "print(script)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jerry: the police? the police have been in there?\n",
            "\n",
            "kramer: yeah, yeah. i mean, the only thing we got in there for a while, they have to wait.\n",
            "\n",
            "george: well, i guess i could say something.... (she leaves)\n",
            "\n",
            "elaine: so she said he was gonna do something like this...\n",
            "\n",
            "jerry: yeah, yeah... (george takes it to his chest)\n",
            "\n",
            "george: what?\n",
            "\n",
            "jerry: you know, you can't get out of here!\n",
            "\n",
            "elaine: oh, i just said it was a little nervous but it's not you. you know, i got a problem.\n",
            "\n",
            "george: what do you want to do this guy in the pool?\n",
            "\n",
            "jerry: what?\n",
            "\n",
            "elaine: what did i do?\n",
            "\n",
            "jerry: well, i'm sure we don't have a baby.\n",
            "\n",
            "elaine: (pause) you know... you can have a little more coffee and a lot of pressure... (points to the table)\n",
            "\n",
            "[setting: dealership car]\n",
            "\n",
            "george: oh, the bubble boy.\n",
            "\n",
            "hoyt: george?\n",
            "\n",
            "peterman: oh, i'm sorry, i'm sure you're going back and i'm gonna see my doctor.\n",
            "\n",
            "jerry: (to george) hey jerry, listen, you know, i'm not really sure that it's like a problem. if you could come down and get it, and you're not going to call me.\n",
            "\n",
            "elaine: why don't you take it out.\n",
            "\n",
            "kramer: no way you know you're a real guy, you can't be dead.\n",
            "\n",
            "jerry: you know, this is what you got?\n",
            "\n",
            "kramer: i got to be a doctor, but i'm sorry. i know i'm gonna go to the bathroom.\n",
            "\n",
            "jerry: yeah.\n",
            "\n",
            "kramer: (leaving his back to the counter) hey, hey, hey kramer, you gotta help.\n",
            "\n",
            "jerry: kramer! (grabs his hand in the back, of his hand)\n",
            "\n",
            "george: i got this one\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mh_0jeX2_p6i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_name = 'generated_script1.txt'\n",
        "with open(file_name, 'w') as file:\n",
        "    file.write(script)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}